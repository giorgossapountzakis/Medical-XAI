# -*- coding: utf-8 -*-
"""extras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B1fwzUO_fIchEZxs0LQUVrvQ36NFJYok

# imports
"""

from __future__ import print_function
from __future__ import division
import numpy as np
import os
import random
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix
import PIL.Image
import matplotlib.cm as cm
from IPython.display import Image, display
from collections import OrderedDict
import cv2
import itertools
import matplotlib.image

import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms, models
import time
import copy
from torchvision.models import *
from collections import OrderedDict
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from skimage.segmentation import slic
# Random seed for reproducibility
seed = 42
random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
np.random.seed(seed)
torch.manual_seed(seed)

labels=['COVID-19','Non-COVID','Normal']

"""# normalization"""

norm_transforms = transforms.Compose([transforms.Resize(299),
                                      transforms.CenterCrop(299),
                                      transforms.ToTensor(),
                                      #transforms.Normalize([0.5, 0.5, 0.5],[0.224, 0.224, 0.224]),
                                       ])

"""# data loading"""

my_transforms = norm_transforms
image_datasets = {x: datasets.ImageFolder('/data/data1/users/el17074/my_full_data/Lung Segmentation Data/'+x, transform=my_transforms) for x in ['Masked_Train','Masked_Test', 'Masked_Val','Masked_Small_Test']}
dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True) for x in ['Masked_Train', 'Masked_Val']}
testdata_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True) for x in ['Masked_Test']}
Small_testdata_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=False) for x in ['Masked_Small_Test']}


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

"""# model selection(change here for fcmiddlenumber and .fc)"""

path='/data/data1/users/el17074/mymodels/inception/inceptionv3hyperparameters/'
fcmiddlenumber=256		
model1= models.inception_v3(weights=Inception_V3_Weights.DEFAULT)
aux_num_ftrs = model1.AuxLogits.fc.in_features
model1.AuxLogits.fc = nn.Linear(aux_num_ftrs, 3)
num_ftrs = model1.fc.in_features
model1.fc =  nn.Sequential(OrderedDict([('fc1', nn.Linear(num_ftrs, fcmiddlenumber)),('relu', nn.ReLU()),('dropout',nn.Dropout()),('fc2', nn.Linear(fcmiddlenumber, 3))]))
model1.load_state_dict(torch.load(path+'weights.pt'))
model1=model1.to(device)
model1.eval()



"""# predictions (change here depending on parameters)"""
print(path)
#1 batch prediction
inputs, classes = next(iter(Small_testdata_dict['Masked_Small_Test']))
rawinputs=inputs
model1 = model1.to(device)
inputs=inputs.to(device)
with torch.no_grad(): 
  outputs=model1(inputs)
  _, preds = torch.max(outputs, 1)
  preds=preds.cpu().numpy()
  classes=classes.numpy()

print(preds)
print(classes)

foundcov=False
foundnoncov=False
foundnormal=False
covposition,noncovposition,normalposition=None,None,None
i=0
for item in classes:
  if foundcov==False or foundnoncov==False or foundnormal==False:
    if classes[i]==0 and foundcov==False:
      covposition=i
      foundcov=True
    elif classes[i]==1 and foundnoncov==False:
      noncovposition=i
      foundnoncov=True
    elif classes[i]==2 and foundnormal==False:
      normalposition=i
      foundnormal=True
    i=i+1
print(covposition,noncovposition,normalposition)

"""# Basic GradCAM"""

from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad
from pytorch_grad_cam import GuidedBackpropReLUModel
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget,RawScoresOutputTarget #an to kaleso me ari8mo eksigi gia tin katigoria ayti
from pytorch_grad_cam.utils.image import show_cam_on_image ,deprocess_image,preprocess_image
from pytorch_grad_cam.metrics.road import *

print('Basic GradCAM')
start_time = time.time()
model = model1
#ALLAZO TO LAYER EDO -----------------------------------------------------------
target_layers = [model.Mixed_7c.branch_pool]
with GradCAM(model=model, target_layers=target_layers,use_cuda=True) as cam:
  im = transforms.ToPILImage()(inputs[covposition]).convert('RGB')
  im = np.asarray(im, dtype="float32" )/255
  input_tensor = inputs
  targets = None #xrisimopoiei ta preds oysiastika
  grayscale_cam= cam(input_tensor=input_tensor, targets=targets,aug_smooth=True,eigen_smooth=True)
  visualization0 = show_cam_on_image(im, grayscale_cam[covposition], use_rgb=True)
  visualization0 = transforms.ToPILImage()(visualization0)
  display(visualization0)




"""# GradCAM metrics"""

def visualize_score(visualization, score, name):
    visualization = cv2.putText(visualization, name, (10, 20), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)
    visualization = cv2.putText(visualization, f"{score:.5f}", (10, 35), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)    
    return visualization

# Now lets see how to evaluate this explanation:
from pytorch_grad_cam.metrics.cam_mult_image import CamMultImageConfidenceChange
from pytorch_grad_cam.utils.model_targets import ClassifierOutputSoftmaxTarget

# For the metrics we want to measure the change in the confidence, after softmax, that's why
# we use ClassifierOutputSoftmaxTarget.
targets = [ClassifierOutputSoftmaxTarget(2)]#analoga tin katigoria poy einai h eikona
cam_metric = CamMultImageConfidenceChange()
scores, visualizations = cam_metric(input_tensor, grayscale_cam, targets, model, return_visualization=True)
score = scores[0]
visualization = visualizations[2].cpu().numpy().transpose((1, 2, 0))
visualization = deprocess_image(visualization)
print(f"The confidence increase: {score}")
visualization=transforms.ToPILImage()(visualization)
visualization = np.array(visualization)
visualization=visualize_score(visualization,score,'confidence increase:')
visualization=transforms.ToPILImage()(visualization)
plt.imshow(visualization)
visualization.save(path+'CamMultImageConfidenceChange.png')
#afairo ta shmeia poy exoyn mikro gradcam syntelesth

from pytorch_grad_cam.sobel_cam import sobel_cam
img=inputs[0].permute(1,2,0).cpu().numpy()

sobel_cam_grayscale = sobel_cam(np.uint8(img * 255))
#thresholded_cam = sobel_cam_grayscale < np.percentile(sobel_cam_grayscale, 75)
cam_metric = CamMultImageConfidenceChange()
scores, visualizations = cam_metric(input_tensor, sobel_cam_grayscale, targets, model, return_visualization=True)
score = scores[0]
#visualization = visualizations[0]
#visualization = deprocess_image(visualization)
visualization = show_cam_on_image(im,sobel_cam_grayscale, use_rgb=True)
visualization=transforms.ToPILImage()(visualization)
visualization = np.array(visualization)
visualization=visualize_score(visualization,score,'confidence increase:')
visualization=transforms.ToPILImage()(visualization)


thresholded_cam = sobel_cam_grayscale < np.percentile(sobel_cam_grayscale, 25)
cam_metric = CamMultImageConfidenceChange()
scores, visualizations = cam_metric(input_tensor, thresholded_cam, targets, model, return_visualization=True)
score = scores[0]
visualization1 = show_cam_on_image(im,thresholded_cam, use_rgb=True)
visualization1=transforms.ToPILImage()(visualization1)
visualization1 = np.array(visualization1)
visualization1=visualize_score(visualization1,score,'confidence increase:')
visualization1=transforms.ToPILImage()(visualization1)

thresholded_cam = sobel_cam_grayscale < np.percentile(sobel_cam_grayscale, 50)
cam_metric = CamMultImageConfidenceChange()
scores, visualizations = cam_metric(input_tensor, thresholded_cam, targets, model, return_visualization=True)
score = scores[0]
visualization2 = show_cam_on_image(im,thresholded_cam, use_rgb=True)
visualization2=transforms.ToPILImage()(visualization2)
visualization2 = np.array(visualization2)
visualization2=visualize_score(visualization2,score,'confidence increase:')
visualization2=transforms.ToPILImage()(visualization2)

thresholded_cam = sobel_cam_grayscale < np.percentile(sobel_cam_grayscale, 75)
cam_metric = CamMultImageConfidenceChange()
scores, visualizations = cam_metric(input_tensor, thresholded_cam, targets, model, return_visualization=True)
score = scores[0]
visualization3 = show_cam_on_image(im,thresholded_cam, use_rgb=True)
visualization3=transforms.ToPILImage()(visualization3)
visualization3 = np.array(visualization3)
visualization3=visualize_score(visualization3,score,'confidence increase:')
visualization3=transforms.ToPILImage()(visualization3)

thresholded_cam = sobel_cam_grayscale < np.percentile(sobel_cam_grayscale, 90)
cam_metric = CamMultImageConfidenceChange()
scores, visualizations = cam_metric(input_tensor, thresholded_cam, targets, model, return_visualization=True)
score = scores[0]
visualization4 = show_cam_on_image(im,thresholded_cam, use_rgb=True)
visualization4=transforms.ToPILImage()(visualization4)
visualization4 = np.array(visualization4)
visualization4=visualize_score(visualization4,score,'confidence increase:')
visualization4=transforms.ToPILImage()(visualization4)

sobel_cam_rgb = cv2.merge([sobel_cam_grayscale, sobel_cam_grayscale, sobel_cam_grayscale])
x=np.hstack((transforms.ToPILImage()(inputs[0]),sobel_cam_rgb, visualization,visualization1,visualization2,visualization3,visualization4))
visualization=transforms.ToPILImage()(x)
display(visualization)
visualization.save(path+'Sobel_Comparison.png')

from pytorch_grad_cam.metrics.road import ROADMostRelevantFirst
cam_metric = ROADMostRelevantFirst(percentile=75)
scores, visualizations = cam_metric(input_tensor, grayscale_cam, targets, model, return_visualization=True)
score = scores[0]
visualization = visualizations[0].cpu().numpy().transpose((1, 2, 0))
visualization = deprocess_image(visualization)
print(f"The confidence increase when removing 25% of the image: {score}")

visualization=transforms.ToPILImage()(visualization)
visualization = np.array(visualization)
visualization=visualize_score(visualization,score,'Conf incr remove 25%:')
visualization=transforms.ToPILImage()(visualization)

cam_metric = ROADMostRelevantFirst(percentile=50)
scores, visualizations = cam_metric(input_tensor, grayscale_cam, targets, model, return_visualization=True)
score = scores[0]
visualization_10 = visualizations[0].cpu().numpy().transpose((1, 2, 0))
visualization_10 = deprocess_image(visualization_10)
print(f"The confidence increase when removing 50% of the image: {score}")
print("The visualizations:")

visualization_10=transforms.ToPILImage()(visualization_10)
visualization_10 = np.array(visualization_10)
visualization_10=visualize_score(visualization_10,score,'Conf incr remove 50%:')
visualization_10=transforms.ToPILImage()(visualization_10)


cam_metric = ROADMostRelevantFirst(percentile=25)
scores, visualizations = cam_metric(input_tensor, grayscale_cam, targets, model, return_visualization=True)
score = scores[0]
visualization_75 = visualizations[0].cpu().numpy().transpose((1, 2, 0))
visualization_75 = deprocess_image(visualization_75)
print(f"The confidence increase when removing 75% of the image: {score}")
print("The visualizations:")

visualization_75=transforms.ToPILImage()(visualization_75)
visualization_75 = np.array(visualization_75)
visualization_75=visualize_score(visualization_75,score,'Conf incr remove 75%:')
visualization_75=transforms.ToPILImage()(visualization_75)

visualization = np.array(visualization)
visualization_10 = np.array(visualization_10)
visualization_75 = np.array(visualization_75)
visualization=np.hstack((visualization,visualization_10, visualization_75))
visualization=transforms.ToPILImage()(visualization)
plt.imshow(visualization)
visualization.save(path+'ROAD.png')

from pytorch_grad_cam.metrics.road import ROADCombined
cam_metric = ROADCombined(percentiles=[25,50,75])
scores = cam_metric(input_tensor, grayscale_cam, targets, model)
print(f"CAM, Combined metric avg confidence increase with ROAD accross 4 thresholds (positive is better): {scores[0]}")
k=open(path+'total ROAD.txt','w')
k.write(f"CAM, Combined metric avg confidence increase with ROAD accross 4 thresholds (positive is better): {scores[0]}")
k.close()

"""# lime """

#https://towardsdatascience.com/how-to-explain-image-classifiers-using-lime-e364097335b4

from lime import lime_image

def get_preprocess_transform():   
    transf = transforms.Compose([
        transforms.ToTensor()
    ])    

    return transf    
preprocess_transform = get_preprocess_transform()


def batch_predict(images):
    model2=model1
    model2.eval()
    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model2 = model2.type(torch.cuda.FloatTensor).to(device)
    batch = batch.type(torch.cuda.FloatTensor).to(device)
    


    logits = model2(batch)
    probs = F.softmax(logits, dim=1)
    return probs.detach().cpu().numpy()

test_pred = batch_predict([np.array(inputs[0].permute(1,2,0).cpu())])
test_pred.squeeze().argmax()

image=np.array(inputs[0].permute(1,2,0).cpu()).astype('double')
#image=image.to(device)
explainer = lime_image.LimeImageExplainer()
explanation = explainer.explain_instance(image,
                                         batch_predict,
                                         top_labels=3, 
                                         hide_color=None, #None
                                         num_samples=10000) # number of images that will be sent to classification function

from skimage.segmentation import mark_boundaries
temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)
img_boundry1 = mark_boundaries(temp, mask)
plt.imshow(img_boundry1)
matplotlib.image.imsave(path+'lime0.png', img_boundry1)

def explanation_heatmap(exp, exp_class):
    dict_heatmap = dict(exp.local_exp[exp_class])
    heatmap = np.vectorize(dict_heatmap.get)(exp.segments) 
    plt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())
    plt.colorbar()
    plt.savefig(path+'lime_heatmap.png')
    plt.title('LIME_heatmap')
    plt.show()


explanation_heatmap(explanation, explanation.top_labels[0])

fig, ax = plt.subplots(1,3, figsize=(25, 25))
for i in range(0,3):
  temp, mask = explanation.get_image_and_mask(explanation.top_labels[i], positive_only=True, num_features=5, hide_rest=False)
  img_boundry = mark_boundaries(temp, mask)
  ax[i].imshow(img_boundry)
  x=i
  ax[i].set_title('Areas that contribute to prediction for class ' +str(i))
  #edo moy deixnei ta top positive
  plt.savefig(path+'lime1.png')

fig, ax = plt.subplots(1,3, figsize=(25, 25))
for i in range(0,3):
  temp, mask = explanation.get_image_and_mask(explanation.top_labels[i], positive_only=False, num_features=5, hide_rest=False)
  img_boundry = mark_boundaries(temp, mask)
  ax[i].imshow(img_boundry)
  x=i
  ax[i].set_title('Areas that contribute to prediction for class ' +str(i))
#edo moy deixnei ta top genika, na paratiriso diafores
plt.savefig(path+'lime2.png')

fig, ax = plt.subplots(1,3, figsize=(25, 25))
for i in range(0,3):
  temp, mask = explanation.get_image_and_mask(explanation.top_labels[i], positive_only=False, num_features=10, hide_rest=True)
  img_boundry = mark_boundaries(temp, mask)
  ax[i].imshow(img_boundry)
  x=i
  ax[i].set_title('Areas that contribute to prediction for class ' +str(i))
#edo moy deixnei ta top genika
plt.savefig(path+'lime3.png')





"""# shapley """

import shap

def nhwc_to_nchw(x: torch.Tensor) -> torch.Tensor:
    if x.dim() == 4:
        x = x if x.shape[1] == 3 else x.permute(0, 3, 1, 2)
    elif x.dim() == 3:
        x = x if x.shape[0] == 3 else x.permute(2, 0, 1)
    return x

def nchw_to_nhwc(x: torch.Tensor) -> torch.Tensor:
    if x.dim() == 4:
        x = x if x.shape[3] == 3 else x.permute(0, 2, 3, 1)
    elif x.dim() == 3:
        x = x if x.shape[2] == 3 else x.permute(1, 2, 0)
    return x


transform= [
    torchvision.transforms.Lambda(nhwc_to_nchw),
    torchvision.transforms.Lambda(nchw_to_nhwc),
]

inv_transform= [
    torchvision.transforms.Lambda(nhwc_to_nchw),
    torchvision.transforms.Lambda(nchw_to_nhwc),
]

transform = torchvision.transforms.Compose(transform)
inv_transform = torchvision.transforms.Compose(inv_transform)

def predict(img: np.ndarray) -> torch.Tensor:
    model2=model1
    model2.eval()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    img = nhwc_to_nchw(torch.Tensor(img))
    img = img.to(device)
    model2=model2.to(device)
    with torch.no_grad(): 
      output = model2(img)
    return output
# Check that transformations work correctly
class_names=['COVID-19','Non-COVID','Normal']
#images=[inputs[0],inputs[1],inputs[12],inputs[13],inputs[22],inputs[23]]
#images=torch.cat(images, dim=0)
Xtr = transform(inputs[0:5])
out = predict(Xtr)
newclasses = torch.argmax(out, axis=1).cpu().numpy()
print(f'Classes: {newclasses}: {np.array(class_names)[newclasses]}')

topk = 3
batch_size = 32
n_evals = 10000


masker_blur = shap.maskers.Image("blur(128,128)", Xtr[0].shape)
explainer = shap.Explainer(predict, masker_blur, algorithm='partition',output_names=class_names)
shap_values = explainer(Xtr[0:5], max_evals=n_evals, batch_size=batch_size,
                        outputs=shap.Explanation.argsort.flip[:topk])
shap_values.data = inv_transform(shap_values.data).cpu().numpy()
shap_values.values = [val for val in np.moveaxis(shap_values.values,-1, 0)]
shap.image_plot(shap_values=shap_values.values,
                pixel_values=shap_values.data,
                labels=shap_values.output_names,
                width=30,aspect=0.4, hspace=0.3,
                )
plt.suptitle('Shapley Partition Explainer')
plt.savefig(path+'ShapleyPartition.png')


x=predict(inputs)
x=x.cpu().numpy()

def change_numbers(arr):
    result = []
    for row in arr:
        sorted_row = np.sort(row)[::-1]
        row_indices = [np.where(row == num)[0][0] for num in sorted_row]
        result.append(row_indices)
    return np.array(result)
y=change_numbers(x)

def change_elements(arr):
    result = np.copy(arr).astype(str)
    result[result == '0'] = 'COVID'
    result[result == '1'] = 'Non-COVID'
    result[result == '2'] = 'Normal'
    return result.astype(str)

names=change_elements(y)




def mask_image(zs, segmentation, image, background=None):
    if background is None:
        background = image.mean((0,1))
    out = np.zeros((zs.shape[0], image.shape[0], image.shape[1], image.shape[2]))
    for i in range(zs.shape[0]):
        out[i,:,:,:] = image
        for j in range(zs.shape[1]):
            if zs[i,j] == 0:
                out[i][segmentation == j,:] = background
    return out#torch.from_numpy(out).clone().detach().requires_grad_(True)

model=model1
def f(z):
    x = mask_image(z, segments_slic, img_array, 255)
    y = model(torch.tensor(x, dtype=torch.float32).permute(0,3,2,1).to(device))
    return y.cpu().detach().numpy()


preds = x
top_preds = y
# make a color map
from matplotlib.colors import LinearSegmentedColormap
colors = []
for l in np.linspace(1,0,100):
    colors.append((245/255,39/255,87/255,l))
for l in np.linspace(0,1,100):
    colors.append((24/255,196/255,93/255,l))
cm = LinearSegmentedColormap.from_list("shap", colors)

def fill_segmentation(values, segmentation):
    out = np.zeros(segmentation.shape)
    for i in range(len(values)):
        out[segmentation == i] = values[i]
    return out

# plot our explanations
fig, axes = plt.subplots(nrows=6, ncols=4, figsize=(15,15))


img=transforms.ToPILImage()(inputs[0])
img_array= np.array(img)#.astype('float')
segments_slic = slic(img, n_segments=10)
explainer = shap.KernelExplainer(f, np.zeros((1,20)))
shap_values = explainer.shap_values(np.ones((1,20)), nsamples=70)
inds = top_preds[0]
axes[0,0].imshow(img)
axes[0,0].axis('off')
max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])
for i in range(3):
    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)
    axes[0,i+1].set_title(names[0][i])
    axes[0,i+1].imshow(img.convert('LA'), alpha=0.15)
    im = axes[0,i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)
    axes[0,i+1].axis('off')



img=transforms.ToPILImage()(inputs[1])
img_array= np.array(img)#.astype('float')
segments_slic = slic(img, n_segments=10)
explainer = shap.KernelExplainer(f, np.zeros((1,20)))
shap_values = explainer.shap_values(np.ones((1,20)), nsamples=70)    
inds = top_preds[1]
axes[1,0].imshow(img)
axes[1,0].axis('off')
max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])
for i in range(3):
    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)
    axes[1,i+1].set_title(names[1][i])
    axes[1,i+1].imshow(img.convert('LA'), alpha=0.15)
    im = axes[1,i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)
    axes[1,i+1].axis('off')



img=transforms.ToPILImage()(inputs[12])
img_array= np.array(img)#.astype('float')
segments_slic = slic(img, n_segments=10)
explainer = shap.KernelExplainer(f, np.zeros((1,20)))
shap_values = explainer.shap_values(np.ones((1,20)), nsamples=70)    
inds = top_preds[12]
axes[2,0].imshow(img)
axes[2,0].axis('off')
max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])
for i in range(3):
    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)
    axes[2,i+1].set_title(names[12][i])
    axes[2,i+1].imshow(img.convert('LA'), alpha=0.15)
    im = axes[2,i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)
    axes[2,i+1].axis('off')

img=transforms.ToPILImage()(inputs[13])
img_array= np.array(img)#.astype('float')
segments_slic = slic(img, n_segments=10)
explainer = shap.KernelExplainer(f, np.zeros((1,20)))
shap_values = explainer.shap_values(np.ones((1,20)), nsamples=70)    
inds = top_preds[13]
axes[3,0].imshow(img)
axes[3,0].axis('off')
max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])
for i in range(3):
    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)
    axes[3,i+1].set_title(names[13][i])
    axes[3,i+1].imshow(img.convert('LA'), alpha=0.15)
    im = axes[3,i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)
    axes[3,i+1].axis('off')

img=transforms.ToPILImage()(inputs[22])
img_array= np.array(img)#.astype('float')
segments_slic = slic(img, n_segments=10)
explainer = shap.KernelExplainer(f, np.zeros((1,20)))
shap_values = explainer.shap_values(np.ones((1,20)), nsamples=70)    
inds = top_preds[22]
axes[4,0].imshow(img)
axes[4,0].axis('off')
max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])
for i in range(3):
    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)
    axes[4,i+1].set_title(names[22][i])
    axes[4,i+1].imshow(img.convert('LA'), alpha=0.15)
    im = axes[4,i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)
    axes[4,i+1].axis('off')

img=transforms.ToPILImage()(inputs[23])
img_array= np.array(img)#.astype('float')
segments_slic = slic(img, n_segments=10)
explainer = shap.KernelExplainer(f, np.zeros((1,20)))
shap_values = explainer.shap_values(np.ones((1,20)), nsamples=70)    
inds = top_preds[23]
axes[5,0].imshow(img)
axes[5,0].axis('off')
max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])
for i in range(3):
    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)
    axes[5,i+1].set_title(names[23][i])
    axes[5,i+1].imshow(img.convert('LA'), alpha=0.15)
    im = axes[5,i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)
    axes[5,i+1].axis('off')







cb = fig.colorbar(im, ax=axes.ravel().tolist(), label="SHAP value", orientation="horizontal", aspect=60)
cb.outline.set_visible(False)
#pl.show()
plt.suptitle('Shapley Kernel Explainer')
plt.savefig(path+'ShapleyKernel.png')






#ALLAZO TO LAYER EDO -----------------------------------------------------------
print('gradient explainer')
to_explain = inputs[0:5]
e = shap.GradientExplainer((model1, model.Mixed_7c.branch_pool), inputs)
shap_values,indexes = e.shap_values(to_explain, ranked_outputs=3, nsamples=2000)
shap_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shap_values]
shap.image_plot(shap_values, to_explain.permute(0,2,3,1).cpu().numpy(),names)
plt.suptitle('Shapley Gradient Explainer')
plt.savefig(path+'ShapleyGradient.png')

